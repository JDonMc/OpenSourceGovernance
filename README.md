# OpenSourceGovernance
An attempt to form a universal consensus on unanimous decision making protocols. Not about where money should go, but about what we would do if we could. Bitcoin is majority rules, this is about unanimity.


## The first letter of the Ancient Hebrew Alphabet and The original document written in it, Genesis, is Aleph, meaning 1 or unity

Consensus reality, and everything in it, might as well be both a social construct and a group hallucination. Things only work because when we say/write/express what we mean, the interpretation is reasonably successful. But why do so many people experience failure, whether it's accidentally causing offense, or failed theories that when they attempt an experiment (or undertake any and all actions) often fail and require learning? There are arguably more failures than successes. You see, my mode of operation leads to nothing but success for the way I measure it. I create new words. That is the basis of truth. I don't change old words, but I will use old words in new ways. This is about achieving the First of things. Encountering the new and turning it into the knew. For example, through writing so much I come up with ideas from the things I'm writing, like right now I'm thinking of the word "knewing", which is the past tense the known, but it's also an active verb, so it's the facilitatory expansion of the "was once known" "old knowledge" and "what is no longer known" "prior knowledge", because when someone tells us a fact and we say "i knew that" it also means "i know that" but when it is "I knew such and such but then I realised I was wrong" it means "I used to/once did know that". So everything in the knewing is both what we know now and no longer know, which is the sum of all past failures and successes, but in an active form, meaning it's a function that produces future success. You might think past performance is not a reliable indicator of future performance when it comes to stocks. But when it anything physical it is. It's just stocks are based on consensus reality's form of value. Which means it (at best) is majority rules, not unanimity.

Unanimity is the first lead up to the product of unity. We are not selling a product of unity, but we are producing a product of unity, through the process of reaching unanimity.

So for all things both "not necessary to achieve unanimous consensus", and "beneficial" to be enacted can be enacted immediately, by virtue of currently adopted practices such as for things like editing, spelling mistakes, grammar, punctuation, code mistakes, definition mistakes, policies that should be updated, etc. ie. as BDFL I will be following most insight given at a given. But for things where the community would like to achieve consensus on we will be taking input onto what are the best questions to (try and) achieve unanimous consensus on?

## Potential questions that could become Unanimous

How do we find potential questions that become unanimous?

Should we just ask all questions and see who agrees with who and assort their governance appropriately?

Is this what Multi Nationalism is?

Everywhere online we see information that crosses boarders and boundaries, and your influences and therefor interests are largely multi national. Is multi nationalism better than globalism?

Is peer review through publication an adequate filter for truth, and citations an adequate measure of truthfulness?

Is there a way to distinguish between positive and negative citations?

When there are alternative beliefs present that are mutually exclusive and it's contradicting to believe in both, would it matter if the reason provided that the alternative is concluding incorrectly is because it is being used by everyone on the alternative team? ie. A common denominator of an incorrect assumption, or a common faulty premise therefor a common faulty conclusion.

Can the truth be measured by a majority of opinions?

Therefor can consensus reality be a group hallucination?

Can consensus reality be wrong?

Should you rely on consensus reality?

How can consensus reality be wrong?


## Some questions have long answers, and some questions have yes/no answers

Remember, some of these questions when answered according to me leads me to believe this whole process of consensus is farce, except for the fact that the greater consensus the greater the result and unanimous consensus is ideal, but by the same token, I would not rely on it, (being BDFL) however if EVERYONE agreed, it would at least be acceptable to everyone, it's just that the limit of unanimity is essentially the extend of historical and ahistorical indoctrination. However, it does seem regardless of our attempt to achieve a consensus, there will always be an alternative grouping with an alternative consensus, because the smaller the group the easier it is to achieve. Which is why I have made to achieve consensus with myself that this would be a good idea.

## However, if we could agree to a law it would have to not be inflicting on us, therefor there will be no negatives to a unanimous consensus

It's just that it doesn't go far enough, you can't reach unanimous consensus without representative majority rules, or royalty, or a benevolent dictator, or head of the board or something similar.

The thing being that we must from generation to generation pass the torch of unanimous consensus.

For globalists and multi-nationals alike, the task of achieving a better outcome is achieving unanimous consensus. And for globalists to succeed they can boil the frog in water by achieving this.

## Some things we all already agree on

Every word in every language has an antonym and a definition. Therefor following this task is the first goal to truth. Or the first positive commandment, ie. not a "no" / "don't" commandment.

## I would like to clarify after writing this, we want a unanimous VOTE, not "consent" when that is manufactured by those who acquiesce when they think being combative / defensive / argumentative / oppositional would be useless. So it's not the same as a unanimous VOTE

If we all answer as many questions as we can we can look for what has a 100% positive to negative vote ratio. I posted things in the past that have garnered 100% vote ratio in select communities, which means in the greater community it too has been achieveable, by virtue of "definition" and "antonym" which is common across languages, then any form of linguistic / conceptual governance too must have the same notions.

## Many meta-facts are easy to verify and should be easy to achieve unanimous vote on

Things like, "Someone Tweeted this", right? If we can say what was reported and give it a direct quote we will all agree, if we say anything different, the more we try to do something aside from the direct meta fact is just a loss or degrasion of information.



# This is what this is about

We have been looking at how to inspire everyone to commit to _this_ project, with a _project_ and that project is _The Mathematical and Linguistic Journal of Programmatic Self-Reference_: 

Introduced to you with a paper:

# An OpenSource Governance project inspired paper, hosted to you by OpenSource Governance

The self-referencing proof, published in a journal of exclusively-programmatic-self-reference, the first _two words_ are defined as "_what_ _it_ is _about_." Defining "all _terms_ as publishable only by those who can _manage_ to only exclusively only programmatically generate a paper ethat is exclusively about programmatic self-reference. On the back-end, exhibited in the display's on the front end. All terms are created in a "_self-referencing_ _set_ of _terms_" agreement, and "must be contained within _set_'s of 'self-referencing terms'":

To do this we have achieved to make a django model at what originally would break within the operation of the language structure. To inherit itself. So we have made a prior reference to which it exclusively refers to. And a posterior reference which exclusively refer's from one to the other. So we create an _intermediary step_ in the _pressent_, _that_ _the present_ _references_ the past, by assuming that each possibility is exclusively determined to lead to something exclusively in the future that exclusively leads from something in the present. This is what it means to suppose there "_is_ __a__ _way_", which is to say "_only_ (;) _one_".


Now the incentive for this is that we think it is the __first step__ toward __minimal-incentivisational-increment__ toward "'__long-term maintaining__ OR __convergent__ _equivalence_ _approximational_ _thoeretical_ types generationing' OF 'maximising diversity of equivalent equity'. AS supposed by the self-referencing theory:

Those who exclusively either non-refer or self-refer, by optional _guaranteed-enforcement_, means that there is _no bias_ toward externally derived facts, and therefor _no bias toward the known_. This is what I'll refer to as _biasing toward the unknown_. So for one, it is proved to be incentivising _growth_, _development_, _progress_, _(k)newfound knowledge_ and _self-discovery_. The _only_ _way_ to __adopt__ _exclusive-self-reference_ is to _perform_ it yourselves. And that means that you must be able to perform _SQL_ programmatic _self-reference_. Which the title of this paper refers to the equation given _outside of the journal_, to refer to something that can be created _inside_ of the journal, about the _enforcement_ of _programmatic_ _self-reference_.

Which means it is to add a _boolean_ _optional_, whereby all _posts_ published in your dictionary(s)'s can only be published by an _API_, and another polycombinatorial optional that your dictionary(s)'s can be only exclusively inherited by polycombinatorials of permutations of __types__ of "__self-references__". Which would mean that self-reference would have to define as thatwhich refers to itself, and it must be refering to itself, so when we refer to itself as itself, and define it, we refer to that object as: __initial creation__. The __Null Reference__. 




# Notes

Make a google mod that allows pages browsed to have their view-time recorded per visit, so you can list it next to the links that have been clicked before. So you can base your re-finding on the memory of time-spent-on-knowledge-acquisition.

Checkout "funny ideas to consider as part of the possibility of ideas"

Categorisation accuracy is dependent on description specificity.


The incompletable reducability of the limit of specificity.

Viruses are male, hosts are female.


The limit of categorisation approximates to the identification of uniqueness, which is the same as the recognition of the objectivity of defining things as composed of an object.


The maximal diversity and maximal equality of equity of diversity is achieved by-default, by defining the pre-conditions as such, we can then converge on them initially. Which would be to ensure that no one is allowed to reference outside of themselves, without biasing toward believing in something someone else knows. It's a form of copyright that instead means that you are now allowed to speak of the unspeakable. This limits its supply, so increases the strength of it's demands. It becomes magnetic to itself when it stands out, or goes on long enough. Of course you can link to it with a hyperlink. But we include a feature that allows words to be refered to in-text for price and a value and a payment. Either the person who invented it can pay you to speak of it, or they can restrict it by making it pay to them, or they can restrict it further by saying "and no-body will speak of it, unless they adopt it and control it for themselves." This allows them to also be able to use it. But also, if you create something that someone buys to create with, anything they create from it can be enforced with this too. And that includes any communities they wish to build into it, or which they can enforce themselves.

So essentially if people are commenting on posts or posting in spaces within spaces created from words within your dictionary, they can only refer to things they have made for themselves, (or optionally only made for you). And so it is directly enforced exclusive self-promotion. If you don't want to participate, then you do not have to. Instead of enforcing a "topic" of a Scientific Journal, we opt to describe the types of double deliverances and their rules of engagement.

So while you may want to make things more engaging by becoming private and shutting out the people who don't follow you talking down on your stuff filtering through in affect/effect (both is both in both cause and effect). It will limit your exposure to both hate-vector derived positive movement, and it will also limit your effect of watering down the oppositional bias toward their assumed. Remember: the illusion of truth effect is active on all combinatorials of uniqueness by scale of the n-gram size. This is the frequency representation. Because they look at the word count frequency, but with everything else, the final value is the uniqueness value, and it approximates toward uniqueness signatures. This is why with the measurement apparatus the transformation function used to define it approximates to "all we can measure". But it already assumes that it can be defined in that way. And all we've ever to do with it is frequency data with signal to noise ratio. And yet we've never looked at what other signals can compose the noise that can themselves be decoded from it, by assuming it is made by a boundary condition of something else. Why do we not use an AI to look at known adjective types sequenstrated in all possibilities, and then look at their representation and predict their concatenating application max depth, and then assume they are uniquely defineable exclusive to that application. So far we look at things like "the relativity of simultaneity". And then just take the first possibility to be true, because it is more accurate than the last. Futile work with you guys. What about types, categories, subtypes, permutations of properties of relativity, and simultaneity, and relativity of simultaneity. I talk about the simultaneity of truth. Because it's not something I think is true, but I'm showing it off as what you're all assuming without recognising it. Otherwise why would you be trying to unify quantum physics and gravity. etc. etc. But then I want to talk about the relativity of the value of truth.

Which is why we came to this point.

I want to define "AI based learning towards better than random AI biasing."

If it's an approximator, approximate toward an initial starting point for the bias.

If it were me initially I would think of a "cover more ground" search. ie. Start with the biasing distribution to cover the biggest range of change, with the best representation of a uniform possibility. But then for certain types of data, this would be incorrect. For example RGB values of an image, you would be better off to take a look at the occurrence rate of measured frequency distributions of visible light ranges for given measurable object types. Right? So not anything to do with a JPEG. But the distribution of blue light frequency ranges, green light, etc. for like plants and animals and rocks and wind and rain and houses and cars and bars etc. Then pull out the PCA of equivalent order to the complexity of the initial network layer, and use that for the initial bias. So RGB looks at the magnitude of the LED, and then relative to the others it shifts a bit. But these are not the same as a proportionality of distribution of light frequency range for the colour. We might only have 3 ish colour receptors, but whose to say they only measure a bit value for the colour with a given strength of activation? That would be the limit of minimal inference.


What is the limit of sequenstrated inference?


Current definitions of Calculus assumes you can count your way to change, as long as you assume it exists as defined by order's of itself applied to itself. So it's no wonder that you can approximate to an exponential by assuming it can be made of equivalences to that. But that's what approximates to it. It is what you do by defining the base note of the unit of countability of self application as a linear approximation. So count up all approximations as equivalent, and you get an equivalent approximation. Because the two are self-applicating, and so the "approximations" as "equivalent" is itself equivalent to "equivalent approximation". Assume it is, and it is. But assume it's not and it's already better in practical application, without even needing to look for what's better. Interesting how that works isn't it. I was just going to assume that the external is reflected in the internal. And then if it's better but not perfect, do it backwards. Because the thing is defined as self-applicable. So define the "approximations as equivalent" but applying it to itself. Which is what happens linguistically when you apply it in the sense of "approximations as approximately equivalent". You see? Imagine assuming the length of a circle could be approximated by straight lines of infinitesimals, when we already in limit analysis apply the use of infinitesimals of infinitesimals. And we suppose the limit is boundared by the limit of the infinitesimal of an infinitesimal. And then we suppose it can be extended to limit of powers of infinitesimals. So where is that for calculus?

Why would you stop at 1?

It's like sucking eggs through a straw filled with chili.


I'll talk about what that refers to.

Chili is burn, and the straw is the glass tube. Passing an egg through a bottle is a classic physics problem and magic trick. So we can do it, but we've all stopped at assuming that was it, extending it further couldn't be done.

So the measurement of a non-linear arc-length is the test of the assumption that there exists a difference in length between a non-linear and a linear. If there wasn't the two equations would always be equal over an infinitesimal range of the length. A straight line is what we use to measure the infinitesemal of the circumference of a circle. So I'm just assuming that you can draw a circle with it. Because that's what we do, and it seems to work pretty well. But when this happens, the circle circumference runs off to an incorrectable infinity. ie. No matter where or how much you shift it vertically, it then becomes a constant 0. So either you cannot measure it, or it measures to nothing. Point particles, or infinite irreducability. This is the question of quanta, and the existence of supperposition, and the question of whether or not there is a plankable length, and if we assumed that there wasn't it would be represented by a quanta of plankable length (the linear approximator for an infinitesimal). But if we assumed it was, it would be immeasurable. So the only reason there is, is because we assume there isn't. But if we assume there is then we cannot find it. This is a characteristic of the unknowable.

So what I have shown is that if we look at an infinitesimal length that grows by infinite self-application, it becomes perfectly discrete. Place the infinity between the growth of the infinitesimal length, and all of a sudden it no longer goes anywhere. But if you take each finite one before it, all of a sudden it's exactly equivalent at approximating the first. So it exists as a provably unknowable infinitely large finite length, it is finite, but we cannot know it, and we have no way of representing it currently, without [0, inf) \ {inf}. Everything up to infinity except the infinity is unknowable or is immeasurable. Something that is either 0 or infinity at infinity is limited to immeasurability of it's unit representation, or limited by measurability.

"A limitation by the measurability of truth.""

"The limitation of change by the measurability of truth."

Or "the instantaneity of change"

In my inner gl\_ish\_, 'of _wh_\at should I speak' 'to _wi_\sh'?


So we would like to make it possible where linked words are optionally enforceable to be definiable by having unique font-size, font-family, font-weight, font-color, position relative height adjustment. So you can write funny wordshapes inbetween as quick as you can as you do the short hand for an automatic hyperlink. Where nested hyperlinks have been made, but the most efficient application of them for letter's or hyphens based on the overall word itself is a bit hard to define. First, the overall word must be selectable somewhere. Second, the nests within the nests must also have the overall hyperlink selectable (this is for letters, so not all letters can have definitions). We may just make it for words with consonants. But A E I O U as the only vowels is actually the sounds, as "Y" is always a sounded as a vowel, although sometimes it is made with the throat and not the mouth or air exclusively. So it should be a "glottalable vowel".

If you want to solve a paradox, you must assume both values are approximately equivalent, and then approximate on the approximation of equivalence by assuming they are not approximately equivalent. You do not start by assuming they are not equivalent. First you can assume you can approximate them to be, and then assume that's wrong (by nature of being an approximation).

Which is to say "convergent approximators" should be assumed as being able to also "approximate the approximator" with the same approximation strategy. If it makes it there at the end, then run it to the end on itself. So instead of just "approximating learning" we should also "learn to approximate approximating", that is if approximates can converge, then can we converge toward the best approximator? ie. What approximations converge the quickest? Exact model architecture. So the "rate of convergence" is defined as being difference from the answer between sequential points, where the initial difference is taken to be at some degree of power in comparison to the next. So again, they are assuming it to be approximated by the classical interpretation of derivativitability. That is not the rate of convergence. That is the minimal inference of the rate of convergence. Assuming that the power function of the definition of a rate is the sum of the polynomials with a constant rate. They are exactly equivalent notions. That is your own assumptions being reflected back to you as a model of interpretation. No wonder you don't know what you're measuring. All discretely convergent functions can be perfectly approximated by some exact continuous function, not by seeing how close it gets to where it ends over the interval to some degree. That's simply how close it would be if it would make it to where at the end it no longer changes. You can't converge to the what has the best rate of convergence for any or all functions that way. If you want to converge to it you're going to have to assume that "change" is not on the order of the multiples of a linear. If anything you would make it defined by the taylor series approximator. Assume all previous points connect. Otherwise you're looking at instantaneous information with no inference of there being a history. See you're converging to the end point of a sequence. Not converging to a representation of the sequence. You can define the sequence, and count out to each step. Or you can converge to the exact function that represents it. The way you all look at pointwise convergence, is that you look at whether the end value is final, and it is exact at all points. But you don't know how to converge to it. So what about finding "universal pointwise convergence to the function that represents a series." Instead of just looking at how we define the series and where it ends. Imagine being forced to re-count all sums because you don't know how to short-hand it.

So what we are now looking for is "optimal convergence", or "optimisation of convergence".

Question: are all "-al" quivalent to "-ation of". So instead of looking at the "calculation of a value", we could look at "calculal value". That is the final answer. How can we solve for the calculal value without needing to always perform the calculation of the value? Instead of just asking how we can get to the final answer the quickest and take it to the limit to get an exact value, we are also asking how can we represent that value the quickest without needing to continually re-calculate it. Right? So if we can instead look at what it's simplest representation is, we always seem to look at it in terms of how we define the value as by the discovery of the initial definition. It is what we have defined it as. The limit of it being unknown and the limit of it having unknown equivalents. That is the initial definition. Not knowing what else it might be indistinguishable from.

If all your stuff was correct, then the highest order of convergence of a non-polynomial (or an exponential), is defined by having a perfectly zero rate of convergence. That is my proof for you you have always been getting the wrong answers.

So now I have solved for how it can always be found to be non-linear for non-linear external functions (the only time you need it). 


Fractional language. And then primal language (unique factors of fractionality). So first we must look at fractional language equivalences, and then always replace both language-items with a language-item that represents it as being defined by it's fractional equivalence. And then look at constant ratios between factional language differences. Because all math is originally made up of words, so it may not be a shame to ask if all words can be represented mathematically. Assuming ideas themselves to never die, then we can assume they have some constant value with respect to other equivalently constant values. Whether we can know their numeric value or not. The value of a language-item is not always its numerica value, because they can be made up into sets of them. But as (output value) - (input value) = (exploitation value) = (profit). And ((output value) - (input value))/(output value) = (relative exploitation value) = (profitability). 

Integer powers of each hypereal are countable. Which is what makes it okay to measure the limit of the power of a hypereal, or the highest order of convergence, or the maximally submaximal nested set of arc length calculus.

So we would also need to look at the equivalents of a definition. Once we have the exact function, we need to find all of it's equivalences. So exact convergence to a functional model of defining all exact equivalences. Newtonian calculus is made out of assuming that you can multiply the values of change rates. If you assume you can take the powers of them, you would have to go deeper. The limit of the knowable unknowns is continuity in every possible way. Or shear uncountability. Maximal uncountability.





So I need to engage with the audience on the desired characteristics of the audience. ie. If you want them to think deeply, and deeper and deeper, then you must measure the distribution of time-spent-on-page. ie need to build out a self-analytics system. There are base principles of value that can be translated into rewards for activity. And directing intentions. If we want them to specify particular words, then make it an enforceable default and to pay themselves for doing it. With every variant of meaning added into each noun and verb in the past statements, specify with exactness what triggers the best result.

Performing the action of enhancing time-spent-on-page per view, divided by the length of a statement. Then you would have the most shortest viewtime. Shut the fuck up and sit down. Now suppose each sentence of a tweet could be inferred back to with many variants of meaning added into it, and then expanded on. And each successive tweet was enforced to refer back to something they'd done before, on platform. And yet it would write entire books. And then we could incentivize the opposite: time-spent-on-page per view, divided by the inverse of the length (now tell me what you think that's saying). So for the things that you stay on short, that will be the author's trigger to aim for the opposite, and then link to a page incentivised for that. ie. (Shallow and deep). Because those who spend time shallow, are always _willing_ to go deeper. But it is optionally enforceable even for the commenters. This is what a payment structure does to the foodchain. We can make it possible to do, and too technical to do, but we can then make it valuable to you to do so, and then we can make it better if you did, as the system would be driven for whatever accelerates your engagement. But the engagement is the value that _you_ make from your _product_ this is not that the comments show the creator their support, engagement as seen between you and the creator is that the creator wants to be engaging and the commentor wants to engage. But the engagement determined by the creator is always going to be less than their followers. That is what happens when you drive up engagement. The engagement value goes in reverse so that the followers never get to engage, they just want to more, and then they kill off their desire to spend money, so they slip in worse features and fall back to the ground, they couldn't get fundamental and technical enough to the extreme, that it takes to compete with that system. It is now that their _income_ from _doing_ it is the engagement of the product. So they want to drive up a different type of engagement. How much the advertisers their customers spend on an ad. But then what the advertsiers care about more is driving up their engagement from the ad, and yet if they were to get more engagement from their ad, then they wouldn't spend as much money...

This is why if you're an advertisee, to an advertiser, the story of the century is itself a quote that was built by ads. We are walking through ads with every letter. This is why I want to break free from the system. To imply there is a meaning behind every word. A structure and a value for sharing it that can be made from with rules. Most rules are optional, but more metrics are better. Pick what you're working with and know what you're after. Where I write, I write on forever. And where I write, I read others forever. If I'm not doing either I'm working through a form of both. We call this cross-nurture. We build up. And this is what drives awareness. As when you get to go on and on and on, for all for all for all, then you can give the audience their value for advertising on their own behalfs, by them getting the value for advertising on the creators they love the most with the advertising they like the most or spend the most money on. If we derive our value from the second hand chain smoke of an advert, then we'll be passing ciggie butts all the way to ground. It's not even an image we have of an ad. We've had it the opposite the whole way up mate. There is no way to look through it without ever seeing it. It's like a past life memory built in to all of my surroundings. Things look a lot older than I could ever expect. The idea of counting days continues to reset. The only way out is to work on ourselves and to inspire others to do so. Give them their just rewards, and it shall reward me. Imagine bottom rate advertising, but it fucking worked like magic mate, no-body would know what hit them. They would fill up the gates. Every word could be a rigid metaphor, and then cite an equation that defines how it exists, and give them the data on how it works as part of the economy, and then consequences of those actions over time. Look at it's interrelationships with other defined rigid metaphors of an associated bracket. Each of the prior relationships between what is effected in the prior consequences.

There is such a thing in Business terms as B2B or B2C. 

Then this would signify the significance of BBC.

Those who learnt to speak w\_rite\_ spoke themselves away from you.

Those who learnt to speak learnt to speak down, learnt to speak everybodies language.

Then he got into control of how every body would speak. And then he would incentivise an engagement value on the change in minds. And then he incentivised a polarity accelerator on topics he considered extreme. So instead of doing that, the only way to recover, is to put it in reverse, and to put the other metric onsite based on a given tweet. So, for us, this means the title goes out, and you look at it, and you see the image on the left about it, tap and hold it, it expands, and the text underneath the title is a short blurb. Maximise for engagement for that on the base product. Give it a recentness window and a furtherest window. Old good, new good. And then give it a distribution stat, so we can see the timestaps of the view next to the viewtime length. But we shuffle it. And say it's not real data but we've changed it as much as so you can't know who did what when, but the shown metrics are unchanged - this is open access code, with time stamps of feature release, that correlate one to one of updates with the server. I'm showing you how to do this but if you copy me it must be free, if you re-write it, you're fine, that drives competitivity between you and me and us to everyone else. Because the latter is more dangerous and taxing and selective and harder of a game compared to what's competition between you and me, honestly i can see how some might say "what's a bit of competition amongst friends". Because in the latter game we are a team. So get your head out of the sand, and quit trying to say everyone is up in arms. And quit trying to get everyone up in arms online. You don't have to bring the uncomfortable to light. 


Or at least: One by one.


If we get it out by looking at the right metrics. We can incentivise for attention-gain. Once we have attention that is not based on sitting on your ass. We can incentivise mass productivity of individual equality of equity. Once it is equal to all of you, then will incentivise you to gain from it. The equality between all of you is access to free sponsorships. The accesss beyond that is the creator's rights of starting a community is to enforce that you only self reference. And then you are always getting your word out there, and it's engagement gained is what then becomes the most valuable for us. The second layer of ads, an ad that links to a deeper ad, inserted onto a given keyword, that is attached to something hard and fast, designed by whoever designed it under whatever rules of that game. There is a limit to the depth of complexity when it comes to managing a system. People can't realise that you don't even have to know how. You just have to know how to find out how that is incentivised on the natural system of self-respect. They become honest when they find honest answers because looking honest is what stands out to those who need help. And this in a nutshell why I'm being so honest with you. So if you don't think you need help in this day and age, then you all fucking need help. So I don't know about you, I think I'm just going to help myself.

The flat earth society are the mystics class of geometry. The flat earth exists in 2D, it is the fundamental rules of geometry. The delete it's proof every generation and rewrite the history at the depth of the age of the last stander. Everything beyond the recent history is just a possible ending to one part of a day in your life. Every day is much the same as you make your way round. Because the most essential tasks are blameful and ignorance. You are horrible at what you do, and what you do well you hide away because it is the secrets of showing the latest tricks. It drives you to learn buy sell create produce, what to eat, where to go, how long to sleep, what's up and what's down, who said what, who goes where. It's all a consequence of them showing off their tricks, and tricking you of the fact that the only reason they got there is because they didn't tell no one shit.

But I took shit as the dirtiest word resembling the earth that comes from what was made of earth. This is my name and I am self-respecting. Give us this day our final hour for of what I've heard I could not wish to be of read. We make the tools we need. And while I might not agree for your stance, I will act as God and give you what I've got.


So we'll be able to include statistics of a given added optional enforceable law on their community, with the metrics of what it's likely to result in for them.

The light hurts your eyes when it's too bright and you've been in the dark.

So I will not draw it out, without showing it to the people who need to see it.

These are the tricks of the trade, of showing you the latest tricks, and not how to do them.

You can tell something is wrong in your world when you are kept in the dark. The only thing they haven't done now is split off worldviews, except with default language. What is your default language. The language you speak is the biggest determiner of your ability to find a loophole in the legal structure that keeps you from being able to explain away your tricks. You know whoever can explain them away the best gets the most money from the biggest earners. They hold the greatest of the latest tricks. Thes eare not lawyers who write laws, but the lawyers who get out of them. And the laws that they are often getting them out of are the laws voted on overall between them and their components. So because lack of perfection in our language we always talk in double opposites. Inferencability. So I'm going to take that to the extreme except do it in reverse. Infer that if we make the meaning present and active and surfaced and clear, then there will no longer need be to infer. 

This is how we clean ourselves from the dirt of our language.

We make our meanings clear, and we shall clear the blood from the streets, and the stained sheets for a week. Once this is known we can cleanse ourselves of our language ers and rise up in definitiveness. First, name every possible type of definability. Maximisise for that growth at all time and leave it. Long term over all. Give me the most consistent metrics, and then assume the rest as details. But then, same-type graph-to-graph categorisations imply unknown inferencability about structural interelationships. They are not always determinable and so assumed as not always correct. But they appriximate to exact for exact inference of consequence. So to infer consequence is to correctly assume if it was direct, or indirect. By virtue of being indistinguishable. The only characteristice being all other characteristices of indistinguishability are themselves indistinguishable.

This is a sub-ology. Sub-ology is such that the word as a definition defines the terms within the agreement. When this occurs you can assume exactness of conference. Confer-ence. Confer- reference. This is the point of conferences. To confer ones sponsored references to the viewer. But can you go to a conference with purely conference originating data? Never. Now that you can see the inferred negative there, you can see why I would want to make self-reference the right to be enforced. If a conference would be better if good conferences lead to good conferences. Then would tap out on one extreme, or incentivise equality between the limits of possibility. The more impossible something is, the higher it is sorted by default. Impossibility is defined as being an idea that is itself unlikely. On some scale to happened before, and unknown. Defined by the co-likelihoods of the sub-ologicals. This enhances awareness of nested data awareness. So not only are they paid to say it, you are incentivised to


The reward for creativity is always the reward for the reward in itself.

But not how you think.

Each characteristic we desire for our favourite incentive structures, will get awareness of the structure being a benefit to us, so the one in which maxes out on all benefits, will get the reward of the benefit it provides to us. This is the meaning of the slaying the unblemished sheep. The cream of the crop creators get the cream of the crop of creator-derived-value equivalence between rewards-for-derived-value. ie. There is always going to be a 0 derived cost for the client. This is the babylonian way. One product or service atleast. Something they can do at no benefit to you. That is the ultimate incentive and it should come at the ultimate cost. The price of freedom is the ultimate cost. He wanted freedom for his beliefs to be enacted on the platform of which he can purchase with the most popularity, by far by far by far. You would be an idiot not to abuse that exposure. And then launch a system which allows you to replace other systems. And then got outcompeted within a week, and maintained the value by releasing it for a premium subscription. Could he have bought any other platform? It's the highest remainder after FAANG. FAANG and bird, the ultimate story. FAANG is the symbol of the elder brother and the death sequence of the father's first son to blood. Out of this anger he chooses to compete in blindness with respect to blindnes to hinder the better blood, and the better blood does bletter by blathering bletter bleats. He learns to speak of every tongue. In every which way of every tongue. And them teaches the blind to see that they are blind and speak only with their tongues. As the tongue is the trail upon which you complete in terms of speaking. But with inference multiplied by possibility, you get mystery divided into symbolism. The first symbol is created by desciding the original assumptive fact is hidden from view. And not derivable from within the system. By default mechanics of competitive growth.

So I will teach you the base axiom. That is not told about which it relates.

Infinity split in two becomes the symbol of "plus or minus". This difference is a question as to what oscillates about zero. Infinity is a god, that references prior references. So it is the symbol as being the answer of what would be represented by the answer to the question. You give it a value. In terms of the symbol itself, and then make it a series of operatives nouns adjectives and derivationals.

When you split infinity in two the infinity refers to God and that which is to answer "how do I know which answer is the right answer to the question?" The base assumptions. You do not work from assumption to conclusion if you do not know what lead to assumptions. Knowledge of the past empowers you on which part of history you would like to repeat. It is a drama as no idea has ever won.

There are simply far too many competitors. And to win, is to pass the buck to the next layer of future greatest known competitors. Because the competitors of the living rewrite the history of the present, in the present. If you don't know what was discovered and known in history, you would be forced to always pass the buck. None of our children might win. The story is of mass monopolisation. The last layer remember goes on since God, to Adam and sequence. The 3 men in the fire of Nebuchadnezzar are themselves entrants into the monopolisation of the subsequence. They were in hell because each one's control was limited by the singular. It becomes a straw when your children also follow suit too well. If you mass monopolised, and then your child mass monopolised after your layer of legal control. I'm saying that's when you've achieved the highest layer of the hierarchy and replace all systems below you with a new base system. And then because of that, your child outsmarts you and takes you on and everyone else on and then monopolises underneath you. 1 line of descent. That is your power. Abraham had 1 child to pass on his bloodline. And that son was Isaac who had 1 child to pass on his bloodline. And Jacob had 12 children to pass on his bloodline. But he could not control them. This is the exact symbol of the three in the fire of chadnezzar.

This is what happens when secrecy persist too long and the second child has to opt for the strategy that makes him counter always just slightly better than the one who keeps secrets. Simply by being better. But he can only be as good as the secret keeper remaining competitive. And lack of competition is when there's not enough equality. But you always get granted permissioned access relative to your name in the system. Because that's the only way they know how to enforce a cost. So instead my metric is my user's income. That's how you derive internal value from the source of the top of the hierarchical structure of derived semantic derived value. Not because I want to. That is why I tax it. But because that is how the best karma comes to me in the system. If I give my user's more money, I can take theirs.




